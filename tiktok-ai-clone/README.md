# AI TikTok Clone

A modern TikTok clone with AI-generated content, featuring an infinite scroll experience similar to TikTok.

## Features

- **AI-Generated Content**: All videos, descriptions, and user profiles are generated by AI
- **Local Video Integration**: Uses local video files to avoid CORS issues and improve development experience
- **GPT-4o Descriptions**: Extracts frames from videos and uses GPT-4o to generate engaging descriptions
- **Infinite Scroll**: Smooth scrolling experience with automatic loading of new content
- **Responsive Design**: Works on both mobile and desktop devices
- **Modern UI**: Clean and intuitive interface inspired by TikTok
- **Analytics Dashboard**: Monitor platform usage, video performance, and user interactions

## Tech Stack

- React
- TypeScript
- Emotion (CSS-in-JS)
- Vite (Build tool)
- React Router (Navigation)
- Framer Motion (Animations)
- OpenAI API (GPT-4o)
- Chart.js (Analytics visualizations)

## Getting Started

### Prerequisites

- Node.js (v14 or higher)
- npm or yarn
- OpenAI API key

### Installation

1. Clone the repository
```bash
git clone <repository-url>
cd tiktok-ai-clone
```

2. Install dependencies
```bash
npm install
```

3. Set up environment variables
Create a `.env` file in the root directory with the following variables:
```
VITE_OPENAI_API_KEY=your_openai_api_key_here
```

### Local Videos

This project uses local video files stored in the `public/assets/videos` directory. The metadata for these videos is stored in `public/assets/data/video_metadata.json`.

If you want to add more videos:
1. Place MP4 video files in the `public/assets/videos` directory
2. Update the `public/assets/data/video_metadata.json` file with metadata for each video
3. Make sure each video entry in the JSON file has a `video_path` property pointing to the correct file path

See `public/assets/README.md` for more details.

4. Start the development server
```bash
npm run dev
```

5. Open your browser and navigate to `http://localhost:3000` (or the port shown in your terminal)

## Project Structure

```
tiktok-ai-clone/
├── src/
│   ├── components/     # Reusable UI components
│   ├── hooks/          # Custom React hooks
│   ├── pages/          # Page components
│   │   ├── Home/       # Main feed page
│   │   └── Dashboard/  # Analytics dashboard
│   ├── services/       # API and service functions
│   │   ├── aiService.ts          # AI content generation
│   │   ├── fineVideoService.ts   # FineVideo integration
│   │   └── openaiService.ts      # OpenAI API integration
│   ├── styles/         # Global styles
│   ├── utils/          # Utility functions
│   ├── App.tsx         # Main App component
│   └── main.tsx        # Entry point
├── public/             # Static assets
│   └── assets/         # Videos, images, and data files
├── index.html          # HTML template
├── package.json        # Dependencies and scripts
├── tsconfig.json       # TypeScript configuration
└── vite.config.ts      # Vite configuration
```

## How It Works

The application integrates with Hugging Face's FineVideo dataset to fetch 15-second video clips. For each video:

1. A frame is extracted from the middle of the clip
2. The frame is sent to OpenAI's GPT-4o API along with video metadata
3. GPT-4o generates an engaging, TikTok-style description for the video
4. AI-generated agent personalities interact with the content

The infinite scroll is implemented using the Intersection Observer API, which detects when the user has scrolled to the bottom of the feed and triggers loading of new content.

## FineVideo Integration

The [FineVideo dataset](https://huggingface.co/datasets/HuggingFaceFV/finevideo) from Hugging Face contains over 43,000 videos with rich metadata including:

- Content categories
- Scene descriptions
- Character information
- Narrative progression
- Mood analysis

### Efficient Data Handling

**Important**: This application does NOT download the entire FineVideo dataset (which is ~600GB). Instead, it:

1. Fetches only a small sample of metadata from the dataset using the Hugging Face API
2. Uses the metadata to generate descriptions with GPT-4o
3. For development purposes, pairs the metadata with sample videos to simulate the experience

This approach allows you to develop and test the application without downloading the entire dataset. When you run `npm run dev`, the app will:

1. Try to fetch metadata from a few FineVideo samples
2. Use sample videos as placeholders for the actual video content
3. Generate descriptions based on the real metadata and frames from the sample videos

If you want to use the actual videos from FineVideo in production, you would need to implement a more sophisticated approach to fetch and process the video content.

## Future Enhancements

- Implement full integration with the Hugging Face API for FineVideo access
- Add video trimming functionality to ensure all clips are exactly 15 seconds
- Implement user authentication and personalized feeds
- Add video recording and uploading functionality
- Enhance agent interactions based on video content
- Implement content recommendation algorithms
- Expand analytics dashboard with more detailed metrics

## License

This project is licensed under the MIT License - see the LICENSE file for details. 